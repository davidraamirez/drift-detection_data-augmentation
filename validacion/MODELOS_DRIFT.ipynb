{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KOLMOGOROV-SMIRNOV TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dataset_drift_ks(base_df,current_df,threshold):\n",
    "  \n",
    "  status = True\n",
    "  report={}\n",
    "  \n",
    "  for column in base_df.columns:\n",
    "    \n",
    "    d1 = base_df[column]\n",
    "    d2 = current_df[column]\n",
    "    is_same_dist = ks_2samp(d1,d2)\n",
    "\n",
    "    if threshold<=is_same_dist.pvalue:\n",
    "      is_found=False\n",
    "    else:\n",
    "      status = False\n",
    "      is_found=True\n",
    "\n",
    "    report.update({column:{\n",
    "    \"p_value\":float(is_same_dist.pvalue),\n",
    "    \"drift_status\":is_found}}) \n",
    "    \n",
    "  return (status,report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STATUS: will return false if there is data drift in any column\n",
    "\n",
    "REPORT: will return a dictionary with a key as an independent feature name. We will get the p-value and drift_status for each feature. If drift_status = True indicates that there is data drift present with respect to that column, which means the training and testing data of the given feature comes from a different distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JENSEN-SHANNON DIVERGENCE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import rel_entr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the continuous data in bins \n",
    "\n",
    "def preparation_data(base_df,current_df,col_name,num_bins=10):\n",
    "    \n",
    "    if base_df[col_name].max() > current_df[col_name].max():\n",
    "        maxi = base_df[col_name].max()\n",
    "    else :\n",
    "        maxi = current_df[col_name].max()\n",
    "    \n",
    "    if base_df[col_name].min() < current_df[col_name].min():\n",
    "        mini = base_df[col_name].min()\n",
    "    else :\n",
    "        mini = current_df[col_name].min()\n",
    "   \n",
    "    bins = np.linspace(mini, maxi, num_bins + 1)\n",
    "    \n",
    "    base_df_copy = base_df.copy()\n",
    "    base_df_copy['bin'] = pd.cut(base_df[col_name],bins=bins,include_lowest=True)\n",
    "    \n",
    "    current_df_copy = current_df.copy()\n",
    "    current_df_copy['bin'] = pd.cut(current_df[col_name],bins=bins,include_lowest=True)\n",
    "    \n",
    "    base_group = base_df_copy.groupby('bin')[col_name].count()/ len(base_df)\n",
    "    current_group = current_df_copy.groupby('bin')[col_name].count() / len(current_df)\n",
    "    \n",
    "    return base_group, current_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Jensen-Shannon divergence between two probability distributions\n",
    "def jensenshannon(p, q, base=None, *, axis=0, keepdims=False):\n",
    "    \n",
    "    m = (p + q) / 2.0\n",
    "    left = rel_entr(p, m)\n",
    "    right = rel_entr(q, m)\n",
    "    left_sum = np.sum(left, axis=axis, keepdims=keepdims)\n",
    "    right_sum = np.sum(right, axis=axis, keepdims=keepdims)\n",
    "    js = left_sum + right_sum\n",
    "    if base is not None:\n",
    "        js /= np.log(base)\n",
    "    return js / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dataset_drift_js(base_df,current_df,threshold,num_bins=10):\n",
    "    \n",
    "    status = True\n",
    "    report={}\n",
    "    \n",
    "    for column in base_df.columns:\n",
    "\n",
    "        \n",
    "        d1,d2=preparation_data(base_df,current_df,column,num_bins)\n",
    "        js =jensenshannon (d1,d2)\n",
    "\n",
    "        if threshold>js:\n",
    "           is_found=False\n",
    "        else:\n",
    "          status = False\n",
    "          is_found=True\n",
    "\n",
    "        report.update({column:{\n",
    "        \"Jensen-Shannon\":float(js),\n",
    "        \"drift_status\":is_found}}) \n",
    "    \n",
    "    return (status,report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STATUS: will return false if there is data drift in any column\n",
    "\n",
    "REPORT: will return a dictionary with a key as an independent feature name. We will get the Jensen-Shannon divergence and drift_status for each feature. If drift_status = True indicates that there is data drift present with respect to that column, which means the training and testing data of the given feature comes from a different distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POPULATION STABILITY INDEX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def population_stability_index(dev_data, val_data,col_name, num_bins=10):\n",
    "    \n",
    "    if dev_data[col_name].max() > val_data[col_name].max():\n",
    "        maxi = dev_data[col_name].max()\n",
    "    else :\n",
    "        maxi = val_data[col_name].max()\n",
    "    \n",
    "    if dev_data[col_name].min() < val_data[col_name].min():\n",
    "        mini = dev_data[col_name].min()\n",
    "    else :\n",
    "        mini = val_data[col_name].min()\n",
    "   \n",
    "    bins = np.linspace(mini, maxi, num_bins + 1)\n",
    "    \n",
    "    dev_data_copy = dev_data.copy()\n",
    "    dev_data_copy['bin'] = pd.cut(dev_data[col_name], bins=bins, include_lowest=True)\n",
    "    \n",
    "    val_data_copy=val_data.copy()\n",
    "    val_data_copy['bin'] = pd.cut(val_data[col_name], bins=bins, include_lowest=True)\n",
    "\n",
    "    dev_group = dev_data_copy.groupby('bin')[col_name].count().reset_index(name='dev_count')\n",
    "    val_group = val_data_copy.groupby('bin')[col_name].count().reset_index(name='val_count')\n",
    "\n",
    "    merged_counts = dev_group.merge(val_group, on='bin', how='left')\n",
    "    \n",
    "    small_constant = 1e-10\n",
    "    merged_counts['dev_pct'] = (merged_counts['dev_count'] / len(dev_data)) + small_constant\n",
    "    merged_counts['val_pct'] = (merged_counts['val_count'] / len(val_data)) + small_constant\n",
    "    merged_counts['psi'] = (merged_counts['val_pct'] - merged_counts['dev_pct']) * np.log(merged_counts['val_pct'] / merged_counts['dev_pct'])\n",
    "\n",
    "    return merged_counts['psi'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dataset_drift_psi(base_df,current_df,threshold,num_bins=10):\n",
    "    \n",
    "    status = True\n",
    "    report={}\n",
    "    for column in base_df.columns:\n",
    "        \n",
    "        psiC = population_stability_index(base_df,current_df,column,num_bins)\n",
    "\n",
    "        if threshold>psiC:\n",
    "           is_found=False\n",
    "        else:\n",
    "          status = False\n",
    "          is_found=True\n",
    "\n",
    "        report.update({column:{\n",
    "        \"PSI\":float(psiC),\n",
    "        \"drift_status\":is_found}}) \n",
    "    \n",
    "    return (status,report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STATUS: will return false if there is data drift in any column\n",
    "\n",
    "REPORT: will return a dictionary with a key as an independent feature name. We will get the PSI and drift_status for each feature. If drift_status = True indicates that there is data drift present with respect to that column, which means the training and testing data of the given feature comes from a different distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_range (input, min, max):\n",
    "    \n",
    "    input += -(np.min(input))\n",
    "    input /= np.max(input) / (max - min)\n",
    "    input += min\n",
    "    \n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_psi(e_perc, a_perc):\n",
    "     \n",
    "    if a_perc == 0:\n",
    "         a_perc = 0.0001\n",
    "    if e_perc == 0:\n",
    "         e_perc = 0.0001\n",
    "\n",
    "    value = (e_perc - a_perc) * np.log(e_perc / a_perc)\n",
    "    \n",
    "    return(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi_quantiles(expected_array, actual_array, buckets):\n",
    "    \n",
    "    breakpoints = np.arange(0, buckets + 1) / (buckets) * 100\n",
    "    breakpoints = np.stack([np.percentile(expected_array, b) for b in breakpoints])\n",
    "\n",
    "    expected_fractions = np.histogram(expected_array, breakpoints)[0] / len(expected_array)\n",
    "    actual_fractions = np.histogram(actual_array, breakpoints)[0] / len(actual_array)\n",
    "    \n",
    "    psi_value = sum(sub_psi(expected_fractions[i], actual_fractions[i]) for i in range(0, len(expected_fractions)))\n",
    "\n",
    "    return psi_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psi_quantiles(expected, actual, buckets=10):\n",
    "    \n",
    "    if len(expected.shape) == 1:\n",
    "        psi_values = np.empty(len(expected.shape))\n",
    "    else:\n",
    "        psi_values = np.empty(expected.shape[1])\n",
    "        \n",
    "    for i in range (0 , len(psi_values) ):\n",
    "        if len(psi_values) == 1:\n",
    "            psi_values = psi_quantiles(expected, actual, buckets)\n",
    "        else:\n",
    "            psi_values[i] = psi_quantiles(expected.iloc[:,i].values, actual.iloc[:,i].values, buckets)\n",
    "    \n",
    "    return psi_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dataset_drift_psi_quantiles(base_df,current_df,threshold,num_quantiles=10):\n",
    "  \n",
    "    status = True\n",
    "    report={}\n",
    "    psi_values = np.empty(base_df.shape[1])\n",
    "    psi_values = calculate_psi_quantiles(base_df,current_df,num_quantiles)\n",
    "    \n",
    "    i=0\n",
    "    for column in base_df.columns:\n",
    "        \n",
    "        if base_df.shape[1]==1:\n",
    "          psiC = psi_values\n",
    "        else:\n",
    "          psiC = psi_values[i]\n",
    "\n",
    "        if threshold>psiC:\n",
    "           is_found=False\n",
    "        else:\n",
    "          status = False\n",
    "          is_found=True\n",
    "\n",
    "        report.update({column:{\n",
    "        \"PSI\":float(psiC),\n",
    "        \"drift_status\":is_found}}) \n",
    "        \n",
    "        i=i+1\n",
    "    \n",
    "    return (status,report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STATUS: will return false if there is data drift in any column\n",
    "\n",
    "REPORT: will return a dictionary with a key as an independent feature name. We will get the PSI and drift_status for each feature. If drift_status = True indicates that there is data drift present with respect to that column, which means the training and testing data of the given feature comes from a different distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUSUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: detecta in ./anaconda3/lib/python3.9/site-packages (0.0.5)\n"
     ]
    }
   ],
   "source": [
    "! pip install detecta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from detecta import detect_cusum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dataset_drift_cusum(df,threshold,drift=0.02):\n",
    "    \n",
    "    status = True\n",
    "    report={}\n",
    "    i=0\n",
    "    \n",
    "    for column in df.columns:\n",
    "        \n",
    "        ta, tai, taf, amp = detect_cusum(df.iloc[:,i].values, threshold, drift, True, True)\n",
    "        \n",
    "        if len(tai)==0:\n",
    "           is_found=False\n",
    "           \n",
    "        else:\n",
    "          status = False\n",
    "          is_found=True\n",
    "\n",
    "        report.update({column:{\n",
    "        \"drift_status\":is_found}}) \n",
    "        i=i+1\n",
    "        \n",
    "    \n",
    "    return (status,report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STATUS: will return false if there is data drift in any column\n",
    "\n",
    "REPORT: will return a dictionary with a key as an independent feature name. We will get the drift_status for each feature. If drift_status = True indicates that there is data drift present with respect to that column, which means the training and testing data of the given feature comes from a different distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def detect_cusum(x, threshold=1, drift=0, ending=False, show=True, ax=None):\n",
    "    \"\"\"Cumulative sum algorithm (CUSUM) to detect abrupt changes in data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 1D array_like\n",
    "        data.\n",
    "    threshold : positive number, optional (default = 1)\n",
    "        amplitude threshold for the change in the data.\n",
    "    drift : positive number, optional (default = 0)\n",
    "        drift term that prevents any change in the absence of change.\n",
    "    ending : bool, optional (default = False)\n",
    "        True (1) to estimate when the change ends; False (0) otherwise.\n",
    "    show : bool, optional (default = True)\n",
    "        True (1) plots data in matplotlib figure, False (0) don't plot.\n",
    "    ax : a matplotlib.axes.Axes instance, optional (default = None).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ta : 1D array_like [indi, indf], int\n",
    "        alarm time (index of when the change was detected).\n",
    "    tai : 1D array_like, int\n",
    "        index of when the change started.\n",
    "    taf : 1D array_like, int\n",
    "        index of when the change ended (if `ending` is True).\n",
    "    amp : 1D array_like, float\n",
    "        amplitude of changes (if `ending` is True).\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRIFT DETECTION METHOD (DDM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "__version__ = '0.5.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import inspect\n",
    "import re\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "_DEFAULT_TAGS = {\n",
    "    'non_deterministic': False,\n",
    "    'requires_positive_data': False,\n",
    "    'X_types': ['2darray'],\n",
    "    'poor_score': False,\n",
    "    'no_validation': False,\n",
    "    'multioutput': False,\n",
    "    \"allow_nan\": False,\n",
    "    'stateless': False,\n",
    "    'multilabel': False,\n",
    "    '_skip_test': False,\n",
    "    'multioutput_only': False}\n",
    "\n",
    "\n",
    "def clone(estimator, safe=True):\n",
    "    \"\"\"Constructs a new estimator with the same parameters.\n",
    "\n",
    "    Clone does a deep copy of the model in an estimator\n",
    "    without actually copying attached data. It yields a new estimator\n",
    "    with the same parameters that has not been fit on any data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object, or list, tuple or set of objects\n",
    "        The estimator or group of estimators to be cloned\n",
    "\n",
    "    safe : boolean, optional\n",
    "        If safe is false, clone will fall back to a deep copy on objects\n",
    "        that are not estimators.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Taken from sklearn for compatibility.\n",
    "    \"\"\"\n",
    "    estimator_type = type(estimator)\n",
    "    # XXX: not handling dictionaries\n",
    "    if estimator_type in (list, tuple, set, frozenset):\n",
    "        return estimator_type([clone(e, safe=safe) for e in estimator])\n",
    "    elif not hasattr(estimator, 'get_params') or isinstance(estimator, type):\n",
    "        if not safe:\n",
    "            return copy.deepcopy(estimator)\n",
    "        else:\n",
    "            raise TypeError(\"Cannot clone object '%s' (type %s): \"\n",
    "                            \"it does not seem to be a valid estimator \"\n",
    "                            \"as it does not implement a 'get_params' methods.\"\n",
    "                            % (repr(estimator), type(estimator)))\n",
    "    klass = estimator.__class__\n",
    "    new_object_params = estimator.get_params(deep=False)\n",
    "    for name, param in new_object_params.items():\n",
    "        new_object_params[name] = clone(param, safe=False)\n",
    "    new_object = klass(**new_object_params)\n",
    "    params_set = new_object.get_params(deep=False)\n",
    "\n",
    "    # quick sanity check of the parameters of the clone\n",
    "    for name in new_object_params:\n",
    "        param1 = new_object_params[name]\n",
    "        param2 = params_set[name]\n",
    "        if param1 is not param2:\n",
    "            raise RuntimeError('Cannot clone object %s, as the constructor '\n",
    "                               'either does not set or modifies parameter %s' %\n",
    "                               (estimator, name))\n",
    "    return new_object\n",
    "\n",
    "\n",
    "def _pprint(params, offset=0, printer=repr):\n",
    "    \"\"\"Pretty print the dictionary 'params'\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : dict\n",
    "        The dictionary to pretty print\n",
    "\n",
    "    offset : int\n",
    "        The offset in characters to add at the begin of each line.\n",
    "\n",
    "    printer : callable\n",
    "        The function to convert entries to strings, typically\n",
    "        the builtin str or repr\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Taken from sklearn for compatibility.\n",
    "    \"\"\"\n",
    "    # Do a multi-line justified repr:\n",
    "    options = np.get_printoptions()\n",
    "    np.set_printoptions(precision=5, threshold=64, edgeitems=2)\n",
    "    params_list = list()\n",
    "    this_line_length = offset\n",
    "    line_sep = ',\\n' + (1 + offset // 2) * ' '\n",
    "    for i, (k, v) in enumerate(sorted(params.items())):\n",
    "        if type(v) is float:\n",
    "            # use str for representing floating point numbers\n",
    "            # this way we get consistent representation across\n",
    "            # architectures and versions.\n",
    "            this_repr = '%s=%s' % (k, str(v))\n",
    "        else:\n",
    "            # use repr of the rest\n",
    "            this_repr = '%s=%s' % (k, printer(v))\n",
    "        if len(this_repr) > 500:\n",
    "            this_repr = this_repr[:300] + '...' + this_repr[-100:]\n",
    "        if i > 0:\n",
    "            if (this_line_length + len(this_repr) >= 75 or '\\n' in this_repr):\n",
    "                params_list.append(line_sep)\n",
    "                this_line_length = len(line_sep)\n",
    "            else:\n",
    "                params_list.append(', ')\n",
    "                this_line_length += 2\n",
    "        params_list.append(this_repr)\n",
    "        this_line_length += len(this_repr)\n",
    "\n",
    "    np.set_printoptions(**options)\n",
    "    lines = ''.join(params_list)\n",
    "    # Strip trailing space to avoid nightmare in doctests\n",
    "    lines = '\\n'.join(line.rstrip(' ') for line in lines.split('\\n'))\n",
    "    return lines\n",
    "\n",
    "\n",
    "def _update_if_consistent(dict1, dict2):\n",
    "    common_keys = set(dict1.keys()).intersection(dict2.keys())\n",
    "    for key in common_keys:\n",
    "        if dict1[key] != dict2[key]:\n",
    "            raise TypeError(\"Inconsistent values for tag {}: {} != {}\".format(\n",
    "                key, dict1[key], dict2[key]\n",
    "            ))\n",
    "    dict1.update(dict2)\n",
    "    return dict1\n",
    "\n",
    "\n",
    "class BaseEstimator:\n",
    "    \"\"\"Base Estimator class for compatibility with scikit-learn.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * All estimators should specify all the parameters that can be set\n",
    "      at the class level in their ``__init__`` as explicit keyword\n",
    "      arguments (no ``*args`` or ``**kwargs``).\n",
    "    * Taken from sklearn for compatibility.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def _get_param_names(cls):\n",
    "        \"\"\"Get parameter names for the estimator\"\"\"\n",
    "        # fetch the constructor or the original constructor before\n",
    "        # deprecation wrapping if any\n",
    "        init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n",
    "        if init is object.__init__:\n",
    "            # No explicit constructor to introspect\n",
    "            return []\n",
    "\n",
    "        # introspect the constructor arguments to find the model parameters\n",
    "        # to represent\n",
    "        init_signature = inspect.signature(init)\n",
    "        # Consider the constructor parameters excluding 'self'\n",
    "        parameters = [p for p in init_signature.parameters.values()\n",
    "                      if p.name != 'self' and p.kind != p.VAR_KEYWORD]\n",
    "        for p in parameters:\n",
    "            if p.kind == p.VAR_POSITIONAL:\n",
    "                raise RuntimeError(\"scikit-multiflow estimators should always \"\n",
    "                                   \"specify their parameters in the signature\"\n",
    "                                   \" of their __init__ (no varargs).\"\n",
    "                                   \" %s with constructor %s doesn't \"\n",
    "                                   \" follow this convention.\"\n",
    "                                   % (cls, init_signature))\n",
    "        # Extract and sort argument names excluding 'self'\n",
    "        return sorted([p.name for p in parameters])\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Get parameters for this estimator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        deep : boolean, optional\n",
    "            If True, will return the parameters for this estimator and\n",
    "            contained subobjects that are estimators.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        params : mapping of string to any\n",
    "            Parameter names mapped to their values.\n",
    "        \"\"\"\n",
    "        out = dict()\n",
    "        for key in self._get_param_names():\n",
    "            value = getattr(self, key, None)\n",
    "            if deep and hasattr(value, 'get_params'):\n",
    "                deep_items = value.get_params().items()\n",
    "                out.update((key + '__' + k, val) for k, val in deep_items)\n",
    "            out[key] = value\n",
    "        return out\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set the parameters of this estimator.\n",
    "\n",
    "        The method works on simple estimators as well as on nested objects\n",
    "        (such as pipelines). The latter have parameters of the form\n",
    "        ``<component>__<parameter>`` so that it's possible to update each\n",
    "        component of a nested object.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        if not params:\n",
    "            # Simple optimization to gain speed (inspect is slow)\n",
    "            return self\n",
    "        valid_params = self.get_params(deep=True)\n",
    "\n",
    "        nested_params = defaultdict(dict)  # grouped by prefix\n",
    "        for key, value in params.items():\n",
    "            key, delim, sub_key = key.partition('__')\n",
    "            if key not in valid_params:\n",
    "                raise ValueError('Invalid parameter %s for estimator %s. '\n",
    "                                 'Check the list of available parameters '\n",
    "                                 'with `estimator.get_params().keys()`.' %\n",
    "                                 (key, self))\n",
    "\n",
    "            if delim:\n",
    "                nested_params[key][sub_key] = value\n",
    "            else:\n",
    "                setattr(self, key, value)\n",
    "                valid_params[key] = value\n",
    "\n",
    "        for key, sub_params in nested_params.items():\n",
    "            valid_params[key].set_params(**sub_params)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __repr__(self, N_CHAR_MAX=700):\n",
    "        # N_CHAR_MAX is the (approximate) maximum number of non-blank\n",
    "        # characters to render. We pass it as an optional parameter to ease\n",
    "        # the tests.\n",
    "\n",
    "        from ..utils._pprint import _EstimatorPrettyPrinter\n",
    "\n",
    "        N_MAX_ELEMENTS_TO_SHOW = 30  # number of elements to show in sequences\n",
    "\n",
    "        # use ellipsis for sequences with a lot of elements\n",
    "        pp = _EstimatorPrettyPrinter(\n",
    "            compact=True, indent=1, indent_at_name=True,\n",
    "            n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n",
    "\n",
    "        repr_ = pp.pformat(self)\n",
    "\n",
    "        # Use bruteforce ellipsis when there are a lot of non-blank characters\n",
    "        n_nonblank = len(''.join(repr_.split()))\n",
    "        if n_nonblank > N_CHAR_MAX:\n",
    "            lim = N_CHAR_MAX // 2  # apprx number of chars to keep on both ends\n",
    "            regex = r'^(\\s*\\S){%d}' % lim\n",
    "            # The regex '^(\\s*\\S){%d}' % n\n",
    "            # matches from the start of the string until the nth non-blank\n",
    "            # character:\n",
    "            # - ^ matches the start of string\n",
    "            # - (pattern){n} matches n repetitions of pattern\n",
    "            # - \\s*\\S matches a non-blank char following zero or more blanks\n",
    "            left_lim = re.match(regex, repr_).end()\n",
    "            right_lim = re.match(regex, repr_[::-1]).end()\n",
    "\n",
    "            if '\\n' in repr_[left_lim:-right_lim]:\n",
    "                # The left side and right side aren't on the same line.\n",
    "                # To avoid weird cuts, e.g.:\n",
    "                # categoric...ore',\n",
    "                # we need to start the right side with an appropriate newline\n",
    "                # character so that it renders properly as:\n",
    "                # categoric...\n",
    "                # handle_unknown='ignore',\n",
    "                # so we add [^\\n]*\\n which matches until the next \\n\n",
    "                regex += r'[^\\n]*\\n'\n",
    "                right_lim = re.match(regex, repr_[::-1]).end()\n",
    "\n",
    "            ellipsis = '...'\n",
    "            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n",
    "                # Only add ellipsis if it results in a shorter repr\n",
    "                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n",
    "\n",
    "        return repr_\n",
    "\n",
    "    def __getstate__(self):\n",
    "        try:\n",
    "            state = super().__getstate__()\n",
    "        except AttributeError:\n",
    "            state = self.__dict__.copy()\n",
    "\n",
    "        if type(self).__module__.startswith('skmultiflow.'):\n",
    "            return dict(state.items(), _skmultiflow_version=__version__)\n",
    "        else:\n",
    "            return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if type(self).__module__.startswith('skmultiflow.'):\n",
    "            pickle_version = state.pop(\"_skmultiflow_version\", \"pre-0.18\")\n",
    "            if pickle_version != __version__:\n",
    "                warnings.warn(\n",
    "                    \"Trying to unpickle estimator {0} from version {1} when \"\n",
    "                    \"using version {2}. This might lead to breaking code or \"\n",
    "                    \"invalid results. Use at your own risk.\".format(\n",
    "                        self.__class__.__name__, pickle_version, __version__),\n",
    "                    UserWarning)\n",
    "        try:\n",
    "            super().__setstate__(state)\n",
    "        except AttributeError:\n",
    "            self.__dict__.update(state)\n",
    "\n",
    "    def _get_tags(self):\n",
    "        collected_tags = {}\n",
    "        for base_class in inspect.getmro(self.__class__):\n",
    "            if (hasattr(base_class, '_more_tags') and base_class != self.__class__):\n",
    "                more_tags = base_class._more_tags(self)\n",
    "                collected_tags = _update_if_consistent(collected_tags,\n",
    "                                                       more_tags)\n",
    "        if hasattr(self, '_more_tags'):\n",
    "            more_tags = self._more_tags()\n",
    "            collected_tags = _update_if_consistent(collected_tags, more_tags)\n",
    "        tags = _DEFAULT_TAGS.copy()\n",
    "        tags.update(collected_tags)\n",
    "        return tags\n",
    "\n",
    "\n",
    "class BaseSKMObject(BaseEstimator):\n",
    "    \"\"\"Base class for most objects in scikit-multiflow\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        This class provides additional functionality not available in the base estimator\n",
    "        from scikit-learn\n",
    "    \"\"\"\n",
    "    def reset(self):\n",
    "        \"\"\" Resets the estimator to its initial state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        # non-optimized default implementation; override if a better\n",
    "        # method is possible for a given object\n",
    "        command = ''.join([line.strip() for line in self.__repr__().split()])\n",
    "        command = command.replace(str(self.__class__.__name__), 'self.__init__')\n",
    "        exec(command)\n",
    "\n",
    "    def get_info(self):\n",
    "        \"\"\" Collects and returns the information about the configuration of the estimator\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        string\n",
    "            Configuration of the estimator.\n",
    "        \"\"\"\n",
    "        return self.__repr__()\n",
    "\n",
    "\n",
    "class ClassifierMixin(metaclass=ABCMeta):\n",
    "    \"\"\"Mixin class for all classifiers in scikit-multiflow.\"\"\"\n",
    "    _estimator_type = \"classifier\"\n",
    "\n",
    "    def fit(self, X, y, classes=None, sample_weight=None):\n",
    "        \"\"\" Fit the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray of shape (n_samples, n_features)\n",
    "            The features to train the model.\n",
    "\n",
    "        y: numpy.ndarray of shape (n_samples, n_targets)\n",
    "            An array-like with the class labels of all samples in X.\n",
    "\n",
    "        classes: numpy.ndarray, optional (default=None)\n",
    "            Contains all possible/known class labels. Usage varies depending\n",
    "            on the learning method.\n",
    "\n",
    "        sample_weight: numpy.ndarray, optional (default=None)\n",
    "            Samples weight. If not provided, uniform weights are assumed.\n",
    "            Usage varies depending on the learning method.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        # non-optimized default implementation; override if a better\n",
    "        # method is possible for a given classifier\n",
    "        self.partial_fit(X, y, classes=classes, sample_weight=sample_weight)\n",
    "\n",
    "        return self\n",
    "\n",
    "    @abstractmethod\n",
    "    def partial_fit(self, X, y, classes=None, sample_weight=None):\n",
    "        \"\"\" Partially (incrementally) fit the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray of shape (n_samples, n_features)\n",
    "            The features to train the model.\n",
    "\n",
    "        y: numpy.ndarray of shape (n_samples)\n",
    "            An array-like with the class labels of all samples in X.\n",
    "\n",
    "        classes: numpy.ndarray, optional (default=None)\n",
    "            Array with all possible/known class labels. Usage varies depending\n",
    "            on the learning method.\n",
    "\n",
    "        sample_weight: numpy.ndarray of shape (n_samples), optional (default=None)\n",
    "            Samples weight. If not provided, uniform weights are assumed.\n",
    "            Usage varies depending on the learning method.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict classes for the passed data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray of shape (n_samples, n_features)\n",
    "            The set of data samples to predict the class labels for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A numpy.ndarray with all the predictions for the samples in X.\n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\" Estimates the probability of each sample in X belonging to each of the class-labels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray of shape (n_samples, n_features)\n",
    "            The matrix of samples one wants to predict the class probabilities for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A numpy.ndarray of shape (n_samples, n_labels), in which each outer entry is associated\n",
    "        with the X entry of the same index. And where the list in index [i] contains\n",
    "        len(self.target_values) elements, each of which represents the probability that\n",
    "        the i-th sample of X belongs to a certain class-label.\n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        \"\"\"Returns the mean accuracy on the given test data and labels.\n",
    "\n",
    "        In multi-label classification, this is the subset accuracy\n",
    "        which is a harsh metric since you require for each sample that\n",
    "        each label set be correctly predicted.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = (n_samples, n_features)\n",
    "            Test samples.\n",
    "\n",
    "        y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
    "            True labels for X.\n",
    "\n",
    "        sample_weight : array-like, shape = [n_samples], optional\n",
    "            Sample weights.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            Mean accuracy of self.predict(X) wrt. y.\n",
    "\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
    "\n",
    "\n",
    "class RegressorMixin(metaclass=ABCMeta):\n",
    "    \"\"\"Mixin class for all regression estimators in scikit-multiflow.\"\"\"\n",
    "    _estimator_type = \"regressor\"\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\" Fit the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray of shape (n_samples, n_features)\n",
    "            The features to train the model.\n",
    "\n",
    "        y: numpy.ndarray of shape (n_samples, n_targets)\n",
    "            An array-like with the target values of all samples in X.\n",
    "\n",
    "        sample_weight: numpy.ndarray, optional (default=None)\n",
    "            Samples weight. If not provided, uniform weights are assumed. Usage varies\n",
    "            depending on the learning method.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        # non-optimized default implementation; override if a better\n",
    "        # method is possible for a given regressor\n",
    "        self.partial_fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "        return self\n",
    "\n",
    "    @abstractmethod\n",
    "    def partial_fit(self, X, y, sample_weight=None):\n",
    "        \"\"\" Partially (incrementally) fit the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray of shape (n_samples, n_features)\n",
    "            The features to train the model.\n",
    "\n",
    "        y: numpy.ndarray of shape (n_samples)\n",
    "            An array-like with the target values of all samples in X.\n",
    "\n",
    "        sample_weight: numpy.ndarray of shape (n_samples), optional (default=None)\n",
    "            Samples weight. If not provided, uniform weights are assumed. Usage varies\n",
    "            depending on the learning method.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict target values for the passed data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray of shape (n_samples, n_features)\n",
    "            The set of data samples to predict the target values for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A numpy.ndarray with all the predictions for the samples in X.\n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\" Estimates the probability for probabilistic/bayesian regressors\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray of shape (n_samples, n_features)\n",
    "            The matrix of samples one wants to predict the probabilities for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        \"\"\"Returns the coefficient of determination R^2 of the prediction.\n",
    "\n",
    "        The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
    "        sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
    "        sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
    "        The best possible score is 1.0 and it can be negative (because the\n",
    "        model can be arbitrarily worse). A constant model that always\n",
    "        predicts the expected value of y, disregarding the input features,\n",
    "        would get a R^2 score of 0.0.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = (n_samples, n_features)\n",
    "            Test samples. For some estimators this may be a\n",
    "            precomputed kernel matrix instead, shape = (n_samples,\n",
    "            n_samples_fitted], where n_samples_fitted is the number of\n",
    "            samples used in the fitting for the estimator.\n",
    "\n",
    "        y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
    "            True values for X.\n",
    "\n",
    "        sample_weight : array-like, shape = [n_samples], optional\n",
    "            Sample weights.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            R^2 of self.predict(X) wrt. y.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The R2 score used when calling ``score`` on a regressor will use\n",
    "        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
    "        with `metrics.r2_score`. This will influence the ``score`` method of\n",
    "        all the multioutput regressors (except for\n",
    "        `multioutput.MultiOutputRegressor`). To specify the default value\n",
    "        manually and avoid the warning, please either call `metrics.r2_score`\n",
    "        directly or make a custom scorer with `metrics.make_scorer` (the\n",
    "        built-in scorer ``'r2'`` uses ``multioutput='uniform_average'``).\n",
    "        \"\"\"\n",
    "\n",
    "        from sklearn.metrics import r2_score\n",
    "        from sklearn.metrics.regression import _check_reg_targets\n",
    "        y_pred = self.predict(X)\n",
    "        # XXX: Remove the check in 0.23\n",
    "        y_type, _, _, _ = _check_reg_targets(y, y_pred, None)\n",
    "        if y_type == 'continuous-multioutput':\n",
    "            warnings.warn(\"The default value of multioutput (not exposed in \"\n",
    "                          \"score method) will change from 'variance_weighted' \"\n",
    "                          \"to 'uniform_average' in 0.23 to keep consistent \"\n",
    "                          \"with 'metrics.r2_score'. To specify the default \"\n",
    "                          \"value manually and avoid the warning, please \"\n",
    "                          \"either call 'metrics.r2_score' directly or make a \"\n",
    "                          \"custom scorer with 'metrics.make_scorer' (the \"\n",
    "                          \"built-in scorer 'r2' uses \"\n",
    "                          \"multioutput='uniform_average').\", FutureWarning)\n",
    "        return r2_score(y, y_pred, sample_weight=sample_weight,\n",
    "                        multioutput='variance_weighted')\n",
    "\n",
    "\n",
    "class MetaEstimatorMixin(object):\n",
    "    \"\"\"Mixin class for all meta estimators in scikit-multiflow.\"\"\"\n",
    "    _required_parameters = [\"estimator\"]\n",
    "\n",
    "\n",
    "class MultiOutputMixin(object):\n",
    "    \"\"\"Mixin to mark estimators that support multioutput.\"\"\"\n",
    "    def _more_tags(self):\n",
    "        return {'multioutput': True}\n",
    "\n",
    "\n",
    "def is_classifier(estimator):\n",
    "    \"\"\"Returns True if the given estimator is (probably) a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object\n",
    "        Estimator object to test.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out : bool\n",
    "        True if estimator is a classifier and False otherwise.\n",
    "    \"\"\"\n",
    "    return getattr(estimator, \"_estimator_type\", None) == \"classifier\"\n",
    "\n",
    "\n",
    "def is_regressor(estimator):\n",
    "    \"\"\"Returns True if the given estimator is (probably) a regressor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object\n",
    "        Estimator object to test.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out : bool\n",
    "        True if estimator is a regressor and False otherwise.\n",
    "    \"\"\"\n",
    "    return getattr(estimator, \"_estimator_type\", None) == \"regressor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDriftDetector(BaseSKMObject, metaclass=ABCMeta):\n",
    "    \"\"\" Abstract Drift Detector\n",
    "    \n",
    "    Any drift detector class should follow this minimum structure in \n",
    "    order to allow interchangeability between all change detection \n",
    "    methods.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    NotImplementedError. All child classes should implement the\n",
    "    get_info function.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    estimator_type = \"drift_detector\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.in_concept_change = None\n",
    "        self.in_warning_zone = None\n",
    "        self.estimation = None\n",
    "        self.delay = None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" reset\n",
    "        \n",
    "        Resets the change detector parameters.\n",
    "         \n",
    "        \"\"\"\n",
    "        self.in_concept_change = False\n",
    "        self.in_warning_zone = False\n",
    "        self.estimation = 0.0\n",
    "        self.delay = 0.0\n",
    "\n",
    "    def detected_change(self):\n",
    "        \"\"\" detected_change\n",
    "        \n",
    "        This function returns whether concept drift was detected or not.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            Whether concept drift was detected or not.\n",
    "        \n",
    "        \"\"\"\n",
    "        return self.in_concept_change\n",
    "\n",
    "    def detected_warning_zone(self):\n",
    "        \"\"\" detected_warning_zone\n",
    "\n",
    "        If the change detector supports the warning zone, this function will return \n",
    "        whether it's inside the warning zone or not.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            Whether the change detector is in the warning zone or not.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.in_warning_zone\n",
    "\n",
    "    def get_length_estimation(self):\n",
    "        \"\"\" get_length_estimation\n",
    "        \n",
    "        Returns the length estimation.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The length estimation\n",
    "        \n",
    "        \"\"\"\n",
    "        return self.estimation\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_element(self, input_value):\n",
    "        \"\"\" add_element\n",
    "        \n",
    "        Adds the relevant data from a sample into the change detector.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_value: Not defined\n",
    "            Whatever input value the change detector takes.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        BaseDriftDetector\n",
    "            self, optional\n",
    "        \n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDM(BaseDriftDetector):\n",
    "    \"\"\" Drift Detection Method.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    min_num_instances: int (default=30)\n",
    "        The minimum required number of analyzed samples so change can be \n",
    "        detected. This is used to avoid false detections during the early \n",
    "        moments of the detector, when the weight of one sample is important.\n",
    "\n",
    "    warning_level: float (default=2.0)\n",
    "        Warning Level\n",
    "\n",
    "    out_control_level: float (default=3.0)\n",
    "        Out-control Level\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    DDM (Drift Detection Method) [1]_ is a concept change detection method\n",
    "    based on the PAC learning model premise, that the learner's error rate\n",
    "    will decrease as the number of analysed samples increase, as long as the\n",
    "    data distribution is stationary.\n",
    "\n",
    "    If the algorithm detects an increase in the error rate, that surpasses\n",
    "    a calculated threshold, either change is detected or the algorithm will\n",
    "    warn the user that change may occur in the near future, which is called\n",
    "    the warning zone.\n",
    "\n",
    "    The detection threshold is calculated in function of two statistics,\n",
    "    obtained when `(pi + si)` is minimum:\n",
    "\n",
    "    * :math:`p_{min}`: The minimum recorded error rate.\n",
    "    * `s_{min}`: The minimum recorded standard deviation.\n",
    "\n",
    "    At instant :math:`i`, the detection algorithm uses:\n",
    "\n",
    "    * :math:`p_i`: The error rate at instant i.\n",
    "    * :math:`s_i`: The standard deviation at instant i.\n",
    "\n",
    "    The conditions for entering the warning zone and detecting change are\n",
    "    as follows:\n",
    "\n",
    "    * if :math:`p_i + s_i \\geq p_{min} + 2 * s_{min}` -> Warning zone\n",
    "    * if :math:`p_i + s_i \\geq p_{min} + 3 * s_{min}` -> Change detected\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_num_instances=30, warning_level=2.0, out_control_level=3.0):\n",
    "        super().__init__()\n",
    "        self.sample_count = None\n",
    "        self.miss_prob = None\n",
    "        self.miss_std = None\n",
    "        self.miss_prob_sd_min = None\n",
    "        self.miss_prob_min = None\n",
    "        self.miss_sd_min = None\n",
    "        self.min_instances = min_num_instances\n",
    "        self.warning_level = warning_level\n",
    "        self.out_control_level = out_control_level\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" reset\n",
    "\n",
    "        Resets the change detector parameters.\n",
    "\n",
    "        \"\"\"\n",
    "        super().reset()\n",
    "        self.sample_count = 1\n",
    "        self.miss_prob = 1.0\n",
    "        self.miss_std = 0.0\n",
    "        self.miss_prob_sd_min = float(\"inf\")\n",
    "        self.miss_prob_min = float(\"inf\")\n",
    "        self.miss_sd_min = float(\"inf\")\n",
    "\n",
    "    def add_element(self, prediction):\n",
    "        \"\"\" Add a new element to the statistics\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        prediction: int (either 0 or 1)\n",
    "            This parameter indicates whether the last sample analyzed was\n",
    "            correctly classified or not. 1 indicates an error (miss-classification).\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        After calling this method, to verify if change was detected or if  \n",
    "        the learner is in the warning zone, one should call the super method \n",
    "        detected_change, which returns True if concept drift was detected and\n",
    "        False otherwise.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.in_concept_change:\n",
    "            self.reset()\n",
    "\n",
    "        self.miss_prob = self.miss_prob + (prediction - self.miss_prob) / float(self.sample_count)\n",
    "        self.miss_std = np.sqrt(self.miss_prob * (1 - self.miss_prob) / float(self.sample_count))\n",
    "        self.sample_count += 1\n",
    "\n",
    "        self.estimation = self.miss_prob\n",
    "        self.in_concept_change = False\n",
    "        self.in_warning_zone = False\n",
    "        self.delay = 0\n",
    "\n",
    "        if self.sample_count < self.min_instances:\n",
    "            return\n",
    "\n",
    "        if self.miss_prob + self.miss_std <= self.miss_prob_sd_min:\n",
    "            self.miss_prob_min = self.miss_prob\n",
    "            self.miss_sd_min = self.miss_std\n",
    "            self.miss_prob_sd_min = self.miss_prob + self.miss_std\n",
    "\n",
    "        if self.miss_prob + self.miss_std > self.miss_prob_min + self.out_control_level * self.miss_sd_min:\n",
    "            self.in_concept_change = True\n",
    "\n",
    "        elif self.miss_prob + self.miss_std > self.miss_prob_min + self.warning_level * self.miss_sd_min:\n",
    "            self.in_warning_zone = True\n",
    "\n",
    "        else:\n",
    "            self.in_warning_zone = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dataset_drift_ddm(model,x,y,min_inst=100,warning=2,threshold=3):\n",
    "  \n",
    "  drift = False\n",
    "  ddm = DDM (min_inst,warning,threshold)\n",
    "  y_pred=model.predict(x)\n",
    "  \n",
    "  for i in range(y.shape[0]):\n",
    "      \n",
    "      y_true = y[i]\n",
    "      \n",
    "      if y_pred[i]==y_true:\n",
    "          ddm.add_element(0)\n",
    "      else:\n",
    "          ddm.add_element(1)\n",
    "      \n",
    "      if ddm.detected_warning_zone():\n",
    "          print('Warning zone has been detected in data of index: ' + str(i))\n",
    "          \n",
    "      if ddm.detected_change():\n",
    "          print('Change has been detected in data of index: ' + str(i))\n",
    "          drift= True \n",
    "          \n",
    "  return drift "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRIFT: will return true if there is model drift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PAGE-HINKLEY TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageHinkley(BaseDriftDetector):\n",
    "    \"\"\" Page-Hinkley method for concept drift detection.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This change detection method works by computing the observed \n",
    "    values and their mean up to the current moment. Page-Hinkley\n",
    "    won't output warning zone warnings, only change detections. \n",
    "    The method works by means of the Page-Hinkley test [1]_. In general\n",
    "    lines it will detect a concept drift if the observed mean at \n",
    "    some instant is greater then a threshold value lambda.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] E. S. Page. 1954. Continuous Inspection Schemes.\n",
    "       Biometrika 41, 1/2 (1954), 100–115.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    min_instances: int (default=30)\n",
    "        The minimum number of instances before detecting change.\n",
    "    delta: float (default=0.005)\n",
    "        The delta factor for the Page Hinkley test.\n",
    "    threshold: int (default=50)\n",
    "        The change detection threshold (lambda).\n",
    "    alpha: float (default=1 - 0.0001)\n",
    "        The forgetting factor, used to weight the observed value \n",
    "        and the mean.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Imports\n",
    "    >>> import numpy as np\n",
    "    >>> from skmultiflow.drift_detection import PageHinkley\n",
    "    >>> ph = PageHinkley()\n",
    "    >>> # Simulating a data stream as a normal distribution of 1's and 0's\n",
    "    >>> data_stream = np.random.randint(2, size=2000)\n",
    "    >>> # Changing the data concept from index 999 to 2000\n",
    "    >>> for i in range(999, 2000):\n",
    "    ...     data_stream[i] = np.random.randint(4, high=8)\n",
    "    >>> # Adding stream elements to the PageHinkley drift detector and verifying if drift occurred\n",
    "    >>> for i in range(2000):\n",
    "    ...     ph.add_element(data_stream[i])\n",
    "    ...     if ph.detected_change():\n",
    "    ...         print('Change has been detected in data: ' + str(data_stream[i]) + ' - of index: ' + str(i))\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, min_instances=30, delta=0.005, threshold=50, alpha=1 - 0.0001):\n",
    "        super().__init__()\n",
    "        self.min_instances = min_instances\n",
    "        self.delta = delta\n",
    "        self.threshold = threshold\n",
    "        self.alpha = alpha\n",
    "        self.x_mean = None\n",
    "        self.sample_count = None\n",
    "        self.sum = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" reset\n",
    "\n",
    "        Resets the change detector parameters.\n",
    "\n",
    "        \"\"\"\n",
    "        super().reset()\n",
    "        self.sample_count = 1\n",
    "        self.x_mean = 0.0\n",
    "        self.sum = 0.0\n",
    "\n",
    "    def add_element(self, x):\n",
    "        \"\"\" Add a new element to the statistics\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: numeric value\n",
    "            The observed value, from which we want to detect the\n",
    "            concept change.\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        After calling this method, to verify if change was detected, one \n",
    "        should call the super method detected_change, which returns True \n",
    "        if concept drift was detected and False otherwise.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.in_concept_change:\n",
    "            self.reset()\n",
    "\n",
    "        self.x_mean = self.x_mean + (x - self.x_mean) / float(self.sample_count)\n",
    "        self.sum = max(0., self.alpha * self.sum + (x - self.x_mean - self.delta))\n",
    "\n",
    "        self.sample_count += 1\n",
    "\n",
    "        self.estimation = self.x_mean\n",
    "        self.in_concept_change = False\n",
    "        self.in_warning_zone = False\n",
    "\n",
    "        self.delay = 0\n",
    "\n",
    "        if self.sample_count < self.min_instances:\n",
    "            return None\n",
    "\n",
    "        if self.sum > self.threshold:\n",
    "            self.in_concept_change = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dataset_drift_ph(df,min_instances=30, delta=0.005, threshold=50, alpha=1 - 0.0001)->bool:\n",
    "  \n",
    "  status = True\n",
    "  report={}\n",
    "  \n",
    "  for column in df.columns:\n",
    "    \n",
    "    d1=df[column]\n",
    "    isFound=False\n",
    "    \n",
    "    pageH = PageHinkley(min_instances,delta,threshold,alpha)\n",
    "    \n",
    "    for i in range(len(d1)):\n",
    "      \n",
    "      pageH.add_element(d1[i])\n",
    "          \n",
    "      if pageH.detected_change():\n",
    "          print('Change has been detected in data: ' + str(d1[i]) + ' - of index: ' + str(i))\n",
    "          status=False\n",
    "          isFound=True\n",
    "          \n",
    "    report.update({column: {\"drift_status\": isFound}})\n",
    "    \n",
    "  return (status,report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STATUS: will return false if there is data drift in any column\n",
    "\n",
    "REPORT: will return a dictionary with a key as an independent feature name. We will get the drift_status for each feature. If drift_status = True indicates that there is data drift present with respect to that column, which means the training and testing data of the given feature comes from a different distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MULTIVARIATE STATISTICAL PROCESS CONTROL (MSPC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mitten\n",
      "  Using cached mitten-0.1.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: numpy in ./anaconda3/lib/python3.9/site-packages (from mitten) (1.21.5)\n",
      "Requirement already satisfied: matplotlib in ./anaconda3/lib/python3.9/site-packages (from mitten) (3.5.1)\n",
      "Requirement already satisfied: pandas in ./anaconda3/lib/python3.9/site-packages (from mitten) (1.4.2)\n",
      "Requirement already satisfied: scipy in ./anaconda3/lib/python3.9/site-packages (from mitten) (1.7.3)\n",
      "Requirement already satisfied: seaborn in ./anaconda3/lib/python3.9/site-packages (from mitten) (0.11.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./anaconda3/lib/python3.9/site-packages (from matplotlib->mitten) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./anaconda3/lib/python3.9/site-packages (from matplotlib->mitten) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./anaconda3/lib/python3.9/site-packages (from matplotlib->mitten) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./anaconda3/lib/python3.9/site-packages (from matplotlib->mitten) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./anaconda3/lib/python3.9/site-packages (from matplotlib->mitten) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./anaconda3/lib/python3.9/site-packages (from matplotlib->mitten) (9.0.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./anaconda3/lib/python3.9/site-packages (from matplotlib->mitten) (4.25.0)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->mitten) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./anaconda3/lib/python3.9/site-packages (from pandas->mitten) (2021.3)\n",
      "Installing collected packages: mitten\n",
      "Successfully installed mitten-0.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install mitten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mase\n",
      "  Using cached mase-0.0.7-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: numpy in ./anaconda3/lib/python3.9/site-packages (from mase) (1.21.5)\n",
      "Requirement already satisfied: matplotlib in ./anaconda3/lib/python3.9/site-packages (from mase) (3.5.1)\n",
      "Requirement already satisfied: seaborn in ./anaconda3/lib/python3.9/site-packages (from mase) (0.11.2)\n",
      "Collecting pyplot-themes\n",
      "  Using cached pyplot_themes-0.2.2-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: pandas in ./anaconda3/lib/python3.9/site-packages (from mase) (1.4.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./anaconda3/lib/python3.9/site-packages (from matplotlib->mase) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./anaconda3/lib/python3.9/site-packages (from matplotlib->mase) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./anaconda3/lib/python3.9/site-packages (from matplotlib->mase) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./anaconda3/lib/python3.9/site-packages (from matplotlib->mase) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./anaconda3/lib/python3.9/site-packages (from matplotlib->mase) (3.0.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./anaconda3/lib/python3.9/site-packages (from matplotlib->mase) (4.25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./anaconda3/lib/python3.9/site-packages (from matplotlib->mase) (9.0.1)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->mase) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./anaconda3/lib/python3.9/site-packages (from pandas->mase) (2021.3)\n",
      "Requirement already satisfied: scipy>=1.0 in ./anaconda3/lib/python3.9/site-packages (from seaborn->mase) (1.7.3)\n",
      "Installing collected packages: pyplot-themes, mase\n",
      "Successfully installed mase-0.0.7 pyplot-themes-0.2.2\n"
     ]
    }
   ],
   "source": [
    "! pip install mase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mitten import mcusum, hotelling_t2, pc_mewma,interpret_multivariate_signal,apply_mewma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCUSUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mcusum(df, num_in_control, k, alpha=0, plotting=True, save='', plot_title='MCUSUM'):\n",
    "\n",
    "\tImplementation of the Multivariate Cumulative Sum (MCUSUM) method.\n",
    "\n",
    "\tArgs:\n",
    "\t\tdf: multivariate dataset as Pandas DataFrame\n",
    "\t\tnum_in_control: number of in control observations\n",
    "\t\tk: the slack parameter which determines model sensetivity (should typically be set to 1/2 of the mean shift that you expect to detect)\n",
    "\t\talpha: the percentage of false positives we want to allow, used for calculating the Upper Control Limit\n",
    "\t\tsave: the directory to save the graphs to, if not changed from default, nothing will be saved\n",
    "\t\tplot_title: the title for the plot generated\n",
    "\tReturns:\n",
    "\t\tMCUSUM statistic values and a calculated UCL with approximately ``alpha`` false positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dataset_drift_mcusum(df,min_inst=100,lambd=0.5):\n",
    "  \n",
    "  y_vals, ucl = mcusum(df,min_inst,lambd)\n",
    "  drift = False\n",
    "  if max(y_vals) > ucl :\n",
    "    drift = True \n",
    "    \n",
    "  return drift "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRIFT: will return TRUE if there is drift "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEWMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pc_mewma(df, num_in_control, num_princ_comps, alpha=0, lambd=0.1, plotting=True, save='', plot_title='PC_MEWMA'):\n",
    "\t\n",
    "\tMEWMA on Principle Components\n",
    "\tVariables contained in ``df`` must have mean 0\n",
    "\n",
    "\tArgs:\n",
    "\t\tdf: multivariate dataset as Pandas DataFrame\n",
    "\t\tnum_in_control: number of in control observations\n",
    "\t\tnum_princ_comps: number of principle components to include\n",
    "\t\talpha: the percentage of false positives we want to allow, used for calculating the Upper Control Limit\n",
    "\t\tlambd: smoothing parameter between 0 and 1; lower value = higher weightage to older observations; default is 0.1\n",
    "\t\tsave: the directory to save the graphs to, if not changed from default, nothing will be saved\n",
    "\t\tplot_title: the title for the plot generated\n",
    "\tReturns:\n",
    "\t\tMEWMA statistic values using PCA for dimensionality reduction and a calculated UCL with approximately ``alpha`` false positive rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply_mewma(df, num_in_control, lambd=0.1, alpha=0, plotting=True, save='', plot_title='MEWMA'):\n",
    "\tArgs:\n",
    "\t\tdf: multivariate dataset as Pandas DataFrame\n",
    "\t\tnum_in_control: number of rows before anomalies begin\n",
    "\t\tlambd: smoothing parameter between 0 and 1; lower value = higher weight to older observations; default is 0.1\n",
    "\t\talpha: the percentage of false positives we want to allow, used for calculating the Upper Control Limit\n",
    "\t\tsave: the directory to save the graphs to, if not changed from default, nothing will be saved\n",
    "\t\tplot_title: the title for the plot generated\n",
    "\tReturns:\n",
    "\t\tMEWMA statistic values and a calculated UCL with approximately ``alpha`` false positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dataset_drift_pC_mewma(df,princ_comp,min_inst=100,lambd=0.1,alpha=0):\n",
    "    \n",
    "    mewma_stats,ucl = pc_mewma(df,min_inst,princ_comp,alpha,lambd)\n",
    "    \n",
    "    drift = False\n",
    "    if max(mewma_stats) > ucl :\n",
    "        drift = True \n",
    "        \n",
    "    return drift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dataset_drift_mewma(df,min_inst=100,lambd=0.1,alpha=0):\n",
    "    \n",
    "    mewma_stats,ucl = apply_mewma(df,min_inst,alpha,lambd)\n",
    "    \n",
    "    drift = False\n",
    "    if max(mewma_stats) > ucl :\n",
    "        drift = True \n",
    "        \n",
    "    return drift "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRIFT: will return TRUE if there is drift "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOTELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hotelling_t2(df, num_in_control, alpha=0, plotting=True, save='', plot_title='Hotellings T^2'):\n",
    "\tArgs:\n",
    "\t\tdf: multivariate dataset as Pandas DataFrame\n",
    "\t\tnum_in_control: number of in control observations before the anomalies start\n",
    "\t\talpha: the percentage of false positives we want to allow, used for calculating the Upper Control Limit\n",
    "\t\tsave: the directory to save the graphs to, if not changed from default, nothing will be saved\n",
    "\t\tplot_title: the title for the plot generated\n",
    "\tReturns:\n",
    "\t\tHotelling T^2 statistic values and a calculated UCL with approximately ``alpha`` false positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dataset_drift_hotelling(df,min_inst=100,alpha=0):\n",
    "    \n",
    "    t2_values,ucl= hotelling_t2(df,min_inst,alpha)   \n",
    "    \n",
    "    drift = False\n",
    "    \n",
    "    if max(t2_values) > ucl :\n",
    "        drift = True \n",
    "        \n",
    "    return drift "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
