{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.interpolate import UnivariateSpline, CubicSpline\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_periodos(inicio, periodos, freq): \n",
    "    serie = pd.date_range(start=inicio, periods=periodos, freq=freq)\n",
    "    return serie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRASLACIÓN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desplazamiento espacial de la serie\n",
    "\n",
    "def traslacion(df,shift,freq):\n",
    "    \n",
    "    df_trasl =df.copy()\n",
    "    for x in df_trasl.columns:\n",
    "        data = df[x]\n",
    "        data_augmented = df[x] + shift\n",
    "        datos = np.concatenate((data.values,data_augmented))\n",
    "        if x == df.columns[0]:\n",
    "            indice = series_periodos(df.index[0],len(datos),freq)\n",
    "            df_trasl = pd.DataFrame(data=datos,index=indice,columns=[x])\n",
    "        else:\n",
    "            df_new = pd.DataFrame(data = datos,index=indice,columns=[x])\n",
    "            df_trasl = df_trasl.join(df_new, how=\"outer\")\n",
    "    return df_trasl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESCALADO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplicación por un factor de la serie\n",
    "\n",
    "def escalado(df,freq,factor):\n",
    "    \n",
    "    df_esc =df.copy()\n",
    "    for x in df_esc.columns:\n",
    "        data = df[x]\n",
    "        data_augmented = df[x]*factor\n",
    "        datos = np.concatenate((data.values,data_augmented))\n",
    "        if x == df.columns[0]:\n",
    "            indice = series_periodos(df.index[0],len(datos),freq)\n",
    "            df_esc = pd.DataFrame(data=datos,index=indice,columns=[x])\n",
    "        else:\n",
    "            df_new = pd.DataFrame(data = datos,index=indice,columns=[x])\n",
    "            df_esc= df_esc.join(df_new, how=\"outer\")\n",
    "    return df_esc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTERPOLACIÓN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos nuevos datos indicando el número de datos a generar, la frequencia y el tipo de interpolación\n",
    "\n",
    "def interpolacion_min_max(df,kind,num,freq):\n",
    "    \n",
    "    df=df.reset_index()\n",
    "    indices=df.index.values\n",
    "    indice=series_periodos(df[df.columns[0]][0],num+df.shape[0],freq)\n",
    "    x = indices \n",
    "    for i in range(1,len(df.columns)):\n",
    "        y = df[df.columns[i]]\n",
    "        inicio = min(df[df.columns[i]].argmin(),df[df.columns[i]].argmax())\n",
    "        fin = max(df[df.columns[i]].argmin(),df[df.columns[i]].argmax())\n",
    "        f = interp1d(x, y, kind=kind) # kind ='linear' / 'cubic' / 'quadratic'\n",
    "        x_new = np.linspace(inicio,fin, num=num)  # New x values\n",
    "        y_new = f(x_new)  # Interpolated y values\n",
    "        if i==1:\n",
    "            df_int = pd.DataFrame(data=np.concatenate((y.values.reshape(-1),y_new)),index=indice,columns=[df.columns[i]])\n",
    "        else :     \n",
    "            df_n = pd.DataFrame(data=np.concatenate((y.values.reshape(-1),y_new)),index=indice,columns=[df.columns[i]])\n",
    "            df_int= df_int.join(df_n, how=\"outer\")\n",
    "            \n",
    "    return df_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos nuevos datos indicando el número de datos a generar, la frequencia y el tipo de interpolación (lineal/cubico).\n",
    "\n",
    "def interpolacion_normal(df,kind,num,freq):\n",
    "    \n",
    "    df=df.reset_index()\n",
    "    indices=df.index.values\n",
    "    indice=series_periodos(df[df.columns[0]][0],num+df.shape[0],freq)\n",
    "    x = indices \n",
    "    for i in range(1,len(df.columns)):\n",
    "        y = df[df.columns[i]]\n",
    "        f = interp1d(x, y, kind=kind) # kind = 'linear' / 'cubic' / 'quadratic'\n",
    "        x_new = np.linspace(0,df.shape[0]-1, num=num)  # New x values\n",
    "        y_new = f(x_new)  # Interpolated y values\n",
    "        if i==1:\n",
    "            df_int = pd.DataFrame(data=np.concatenate((y.values.reshape(-1),y_new)),index=indice,columns=[df.columns[i]])\n",
    "        else :     \n",
    "            df_n = pd.DataFrame(data=np.concatenate((y.values.reshape(-1),y_new)),index=indice,columns=[df.columns[i]])\n",
    "            df_int= df_int.join(df_n, how=\"outer\")\n",
    "            \n",
    "    return df_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(data):\n",
    "    interpolated_data = []\n",
    "    for i in range(len(data) - 1):\n",
    "        interpolated_data.append(data[i])\n",
    "        interpolated_data.append((data[i] + data[i + 1]) / 2)  # Punto intermedio\n",
    "    interpolated_data.append(data[-1])\n",
    "    return np.array(interpolated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadimos datos que sean el punto de medio entre dos datos consecutivos\n",
    "\n",
    "def punto_medio(df,freq):\n",
    "    \n",
    "    for x in df.columns:\n",
    "        data = df[x]\n",
    "        a = interpolate(data)\n",
    "        if x == df.columns[0]:\n",
    "            indice = series_periodos(df.index[0],len(a),freq)\n",
    "            df_pm = pd.DataFrame(data=a,index=indice,columns=[x])\n",
    "        else:\n",
    "            df_new = pd.DataFrame(data=a,index=indice,columns=[x])\n",
    "            df_pm = df_pm.join(df_new, how=\"outer\")\n",
    "    return df_pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spline_interpolation_linear(data, num,s=1):\n",
    "    x = np.arange(len(data))\n",
    "    spline = UnivariateSpline(x, data, s=s)\n",
    "    x_new = np.linspace(0,len(data)-1, num=num)\n",
    "    return spline(x_new)\n",
    "\n",
    "def spline_interpolation_cubic(data, num):\n",
    "    x = np.arange(len(data))\n",
    "    spline = CubicSpline(x,data)\n",
    "    x_new = np.linspace(0,len(data)-1, num=num)\n",
    "    return spline(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolacion_spline(df,tipo,num,freq,s):\n",
    "    \n",
    "    indice=series_periodos(df.index[0],num+df.shape[0],freq)\n",
    "    for x in df.columns:\n",
    "        y=df[x]\n",
    "        if tipo=='linear': \n",
    "            y_new = spline_interpolation_linear(df[x],num,s)\n",
    "        elif tipo=='cubic':\n",
    "            y_new = spline_interpolation_cubic(df[x],num)\n",
    "        if x==df.columns[0]:\n",
    "            df_int = pd.DataFrame(data=np.concatenate((y.values.reshape(-1),y_new)),index=indice,columns=[x])\n",
    "        else :     \n",
    "            df_n = pd.DataFrame(data=np.concatenate((y.values.reshape(-1),y_new)),index=indice,columns=[x])\n",
    "            df_int= df_int.join(df_n, how=\"outer\")       \n",
    "    return df_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAMPLING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sampling with replacement\n",
    "\n",
    "def sampling(df,size,freq):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    indice = series_periodos(df.index[0],size+df.shape[0],freq)\n",
    "    for x in df.columns:\n",
    "        data = df[x]\n",
    "        sampled_data = np.random.choice(data, size=size, replace=True) + np.random.normal(0, 0.5, size)\n",
    "        if x == df.columns[0]:\n",
    "            df_sampling=pd.DataFrame(data=np.concatenate((data,sampled_data)),index=indice,columns=[x])\n",
    "        else:\n",
    "            df_new = pd.DataFrame(data=np.concatenate((data,sampled_data)),index=indice,columns=[x])\n",
    "            df_sampling= df_sampling.join(df_new, how=\"outer\")\n",
    "    return df_sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRANSFORMACIONES MATEMÁTICA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos log\n",
    "def agregar_log(df):\n",
    "    df_o = df.copy()\n",
    "    for x in df_o.columns:\n",
    "        df_o[x] = np.log1p(df[x])\n",
    "    return df_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos la raíz cuadrada \n",
    "def agregar_sqrt(df):\n",
    "    df_o = df.copy()\n",
    "    for x in df_o.columns:\n",
    "        df_o[x] = np.sqrt(df[x])\n",
    "    return df_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos la exponencial\n",
    "def agregar_exp(df,factor):\n",
    "    df_o = df.copy()\n",
    "    for x in df_o.columns:\n",
    "        df_o[x] = np.exp(df_o[x]/factor)\n",
    "    return df_o "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos el seno\n",
    "def agregar_sin(df):\n",
    "    df_o = df.copy()\n",
    "    for x in df_o.columns:\n",
    "        df_o[x] = np.sin(df[x])\n",
    "    return df_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos sen + cos\n",
    "\n",
    "def agregar_trig(df):\n",
    "    df_o = df.copy()\n",
    "    for x in df_o.columns:\n",
    "        df_o[x] = np.cos(df_o[x]) + np.sin(df_o[x])\n",
    "    return df_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos sigmoide\n",
    "\n",
    "def agregar_sigmoid(df):\n",
    "    df_o = df.copy()\n",
    "    for x in df.columns:\n",
    "        df_o[x] = 1 / (1 + np.exp(-df_o[x]))\n",
    "    return df_o\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos operaciones matemáticas\n",
    "\n",
    "def agregar_matematica(df,freq,funcion,factor=1):\n",
    "    \n",
    "    indice=series_periodos(df.index[0],2*df.shape[0],freq)\n",
    "    for x in df.columns:\n",
    "        data = df[x]\n",
    "        if funcion == 'sqrt':\n",
    "            transformed_data = np.sqrt(data)\n",
    "        elif funcion == 'log':\n",
    "            transformed_data = np.log1p(data)\n",
    "        elif funcion == 'exp':\n",
    "            transformed_data = np.exp(data/factor)\n",
    "        elif funcion == 'sin':\n",
    "            transformed_data = np.sin(data)\n",
    "        elif funcion == 'cos':\n",
    "            transformed_data = np.cos(data)\n",
    "        elif funcion == 'trig':\n",
    "            transformed_data = np.cos(data) + np.sin(data)\n",
    "        elif funcion == 'sigmoide':\n",
    "            transformed_data = 1 / (1 + np.exp(-data))\n",
    "\n",
    "        if x == df.columns[0]:\n",
    "            df_transf=pd.DataFrame(data=np.concatenate((data,transformed_data)),index=indice,columns=[x])\n",
    "        else:\n",
    "            df_new = pd.DataFrame(data=np.concatenate((data,transformed_data)),index=indice,columns=[x])\n",
    "            df_transf= df_transf.join(df_new, how=\"outer\")\n",
    "    return df_transf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TÉCNICAS ESTADÍSTICAS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estadist(df,freq,num,tipo):\n",
    "    \n",
    "    indice=series_periodos(df.index[0],num+df.shape[0],freq)\n",
    "    for x in df.columns:\n",
    "        data = df[x]\n",
    "        if tipo==1:\n",
    "            transformed_data = np.zeros(num)+ data.mean()\n",
    "        elif tipo==2:\n",
    "            transformed_data = np.zeros(num) + data.median()\n",
    "        elif tipo==3:\n",
    "            transformed_data = np.zeros(num) + data.mode().iloc[0]\n",
    "\n",
    "        if x == df.columns[0]:\n",
    "            df_transf=pd.DataFrame(data=np.concatenate((data,transformed_data)),index=indice,columns=[x])\n",
    "        else:\n",
    "            df_new = pd.DataFrame(data=np.concatenate((data,transformed_data)),index=indice,columns=[x])\n",
    "            df_transf= df_transf.join(df_new, how=\"outer\")\n",
    "            \n",
    "    return df_transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devuelve df con datos añadidos calculados a partir de una distribución normal con la media y desviación de los datos pasados \n",
    "\n",
    "def normal(df,freq,size):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    indice=series_periodos(df.index[0],size+df.shape[0],freq)\n",
    "    for x in df.columns:\n",
    "        data = df[x]\n",
    "        mean,std_dev = np.mean(data),np.std(data)\n",
    "        data_augmented = np.random.normal(mean,std_dev,size=size)\n",
    "        if x == df.columns[0]:\n",
    "            df_normal=pd.DataFrame(data=np.concatenate((df[x].values,data_augmented)),index=indice,columns=[x])\n",
    "        else:\n",
    "            df_new = pd.DataFrame(data=np.concatenate((data,data_augmented)),index=indice,columns=[x])\n",
    "            df_normal= df_normal.join(df_new, how=\"outer\")\n",
    "    return df_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula nuevos datos usando: media + z * desv donde la media y las desv son las de los datos pasados y z = raiz (-2 * log u1) cos(2 pi u2) tal que u1,u2 son dos randoms entre 0 e 1\n",
    "def box_muller_transform(mean, std_dev, size=100):\n",
    "    u1, u2 = np.random.rand(size), np.random.rand(size)\n",
    "    z1 = np.sqrt(-2 * np.log(u1)) * np.cos(2 * np.pi * u2)\n",
    "    return mean + z1 * std_dev\n",
    "\n",
    "def box_muller(df,freq,size):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    indice=series_periodos(df.index[0],size+df.shape[0],freq)\n",
    "    for x in df.columns:\n",
    "        data = df[x].values\n",
    "        data_bm = box_muller_transform(data.mean(),data.std(),size)\n",
    "        if x == df.columns[0]:\n",
    "            df_bm=pd.DataFrame(data=np.concatenate((df[x].values,data_bm)),index=indice,columns=[x])\n",
    "        else:\n",
    "            df_new = pd.DataFrame(data=np.concatenate((df[x].values,data_bm)),index=indice,columns=[x])\n",
    "            df_bm = df_bm.join(df_new, how=\"outer\")\n",
    "    return df_bm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUIDO ARMONICO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadimos ruido armonico a la muestra con cierta amplitud y frequencia\n",
    "\n",
    "def add_harmonic_noise(df,freq,size):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    df_harm = df.copy()\n",
    "    for x in df_harm.columns:\n",
    "        data = df[x]\n",
    "        time = np.arange(size)\n",
    "        # Aplicar FFT\n",
    "        fft_result = np.fft.fft(data)\n",
    "        frequencies = np.fft.fftfreq(len(data), d=(time[1] - time[0]))  # Frecuencias asociadas\n",
    "        amplitudes = np.abs(fft_result)  # Magnitudes (amplitud)\n",
    "        dominant_freq_idx = np.argmax(amplitudes)\n",
    "        frequency = frequencies[dominant_freq_idx]\n",
    "        amplitude = amplitudes[dominant_freq_idx]\n",
    "        harmonic_noise = amplitude * np.sin(2 * np.pi * frequency * time)\n",
    "        data_augmented = np.random.choice(data, size=size, replace=True) + harmonic_noise\n",
    "        datos = np.concatenate((data.values,data_augmented))\n",
    "        if x == df.columns[0]:\n",
    "            indice = series_periodos(df.index[0],len(datos),freq)\n",
    "            df_harm = pd.DataFrame(data=datos,index=indice,columns=[x])\n",
    "        else:\n",
    "            df_new = pd.DataFrame(data = datos,index=indice,columns=[x])\n",
    "            df_harm = df_harm.join(df_new, how=\"outer\")\n",
    "    \n",
    "    return df_harm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DUPLICADO + PERTURBACIÓN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicar algunos datos y añadir ruido\n",
    "def duplicate_and_perturb(data, duplication_factor=0.3, perturbation_std=0.05):\n",
    "    duplicated_data = []\n",
    "    np.random.seed(8)\n",
    "    for point in data:\n",
    "        duplicated_data.append(point)\n",
    "        if np.random.rand() < duplication_factor:\n",
    "            duplicated_data.append(point + np.random.normal(0, perturbation_std))\n",
    "    return np.array(duplicated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicamos algunos datos añadiendole cierto ruido.\n",
    "\n",
    "def duplicados(df,freq,duplication_factor=0.3,perturbation_std=0.05):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    for x in df.columns:\n",
    "        data = df[x]\n",
    "        data_dd=duplicate_and_perturb(data,duplication_factor,perturbation_std)\n",
    "        if x == df.columns[0]:\n",
    "            indice = series_periodos(df.index[0],len(data_dd),freq)\n",
    "            df_dd = pd.DataFrame(data=data_dd,index=indice,columns=[x])\n",
    "        else:\n",
    "            df_new = pd.DataFrame(data = data_dd,index=indice,columns=[x])\n",
    "            df_dd = df_dd.join(df_new, how=\"outer\")\n",
    "            \n",
    "    return df_dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMBINACIÓN LINEAL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos nuevos datos como combinación lineal de los otros \n",
    "def linear_combinations(data,num_datos, n_combinations):\n",
    "    for _ in range(num_datos):\n",
    "        datos = data[-n_combinations:]\n",
    "        weights = np.random.rand(n_combinations)\n",
    "        weights /= np.sum(weights)  # Normalizar pesos\n",
    "        combination = np.dot(weights, datos)\n",
    "        combination += np.random.normal(0,0.5)\n",
    "        data=np.append(data,combination)\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agregar_comb(df,freq,size,window_size):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    for x in df.columns:\n",
    "        data = df[x]\n",
    "        datos = linear_combinations(data.values,size,window_size)\n",
    "        if x == df.columns[0]:\n",
    "            indice = series_periodos(df.index[0],len(datos),freq)\n",
    "            df_dl = pd.DataFrame(data=datos,index=indice,columns=[x])\n",
    "        else:\n",
    "            df_new = pd.DataFrame(data = datos,index=indice,columns=[x])\n",
    "            df_dl = df_dl.join(df_new, how=\"outer\")\n",
    "    return df_dl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DESCOMPOSICIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descomp(df,size,freq,tipo):\n",
    "    \n",
    "    indice=series_periodos(df.index[0],size+df.shape[0],freq)\n",
    "\n",
    "    for x in df.columns:\n",
    "        data = df[x]\n",
    "        # Descomposición de la serie\n",
    "        if tipo==\"additive\":\n",
    "            descomposicion = seasonal_decompose(data, model='additive', period=12)\n",
    "        elif tipo==\"multiplicative\":\n",
    "            descomposicion = seasonal_decompose(data, model='multiplicative', period=12)\n",
    "            \n",
    "        tendencia = descomposicion.trend\n",
    "        estacionalidad = descomposicion.seasonal\n",
    "        residuo = descomposicion.resid\n",
    "        # Calcular la tasa de cambio promedio de la tendencia\n",
    "        tendencia_valida = tendencia.dropna()\n",
    "        cambios = tendencia_valida.diff().dropna()\n",
    "        tasa_cambio_promedio = cambios.mean()\n",
    "\n",
    "        # Extrapolar los valores de la tendencia\n",
    "        n_pasos = size\n",
    "        ultima_tendencia = tendencia_valida.iloc[-1]\n",
    "        tendencia_futura = [ultima_tendencia + (i + 1) * tasa_cambio_promedio for i in range(n_pasos)]\n",
    "        \n",
    "        # Replicar los valores estacionales\n",
    "        longitud_estacionalidad = 12  # Basado en la periodicidad detectada\n",
    "        estacionalidad_extrapolada = np.tile(estacionalidad[-longitud_estacionalidad:], int(size/12)+1)[:size]\n",
    "        if tipo==\"additive\":\n",
    "            prediccion = tendencia_futura + estacionalidad_extrapolada\n",
    "        elif tipo==\"multiplicative\":\n",
    "            prediccion = tendencia_futura * estacionalidad_extrapolada\n",
    "        if x == df.columns[0]:\n",
    "            df_desc=pd.DataFrame(data=np.concatenate((data,prediccion)),index=indice,columns=[x])\n",
    "        else:\n",
    "            df_new = pd.DataFrame(data=np.concatenate((data,prediccion)),index=indice,columns=[x])\n",
    "            df_desc= df_desc.join(df_new, how=\"outer\")\n",
    "    return df_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELOS DE PREDICCIÓN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skforecast.Sarimax import Sarimax\n",
    "from skforecast.ForecasterSarimax import ForecasterSarimax\n",
    "from skforecast.model_selection_sarimax import backtesting_sarimax\n",
    "from skforecast.model_selection_sarimax import grid_search_sarimax\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from skforecast.model_selection import grid_search_forecaster\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\n",
    "from sklearn.linear_model import Ridge\n",
    "from prophet import Prophet\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de modelo autorregresivos con búsqueda de parámetros realizada por grid search devolviendo la predicción\n",
    "def prediccion_sarimax(datos,datos_train,columna,size):\n",
    "    \n",
    "    # Grid search\n",
    "    forecaster = ForecasterSarimax(\n",
    "                    regressor=Sarimax(\n",
    "                                    order=(1, 1, 1), # Placeholder replaced in the grid search\n",
    "                                    maxiter=500\n",
    "                                )\n",
    "                )\n",
    "\n",
    "    param_grid = {\n",
    "        'order': [(0, 1, 0), (0, 1, 1), (1, 1, 0), (1, 1, 1), (2, 1, 1), (1 ,1 ,2), ( 2, 1, 2),(0, 0, 0), (0, 0, 1), (1, 0, 0), (1, 0, 1), (2, 0, 1), (1 ,0 ,2), ( 2, 0, 2) ],\n",
    "        'seasonal_order': [(0, 0, 0, 0), (0, 1, 0, 12), (1, 1, 1, 12)],\n",
    "        'trend': [None]\n",
    "    }\n",
    "\n",
    "    resultados_grid = grid_search_sarimax(\n",
    "                            forecaster            = forecaster,\n",
    "                            y                     = datos[columna],\n",
    "                            param_grid            = param_grid,\n",
    "                            steps                 = 12,\n",
    "                            refit                 = True,\n",
    "                            metric                = 'mean_absolute_error',\n",
    "                            initial_train_size    = int(len(datos_train)*0.8),\n",
    "                            fixed_train_size      = False,\n",
    "                            return_best           = False,\n",
    "                            n_jobs                = 'auto',\n",
    "                            suppress_warnings_fit = True,\n",
    "                            verbose               = False,\n",
    "                            show_progress         = True\n",
    "                    )\n",
    "    \n",
    "    r=resultados_grid.index[0]\n",
    "\n",
    "    # Predicciones de backtesting con el mejor modelo según el grid search\n",
    "    # ==============================================================================\n",
    "    forecaster_1 = ForecasterSarimax( regressor=Sarimax(order=resultados_grid.order[r], seasonal_order=resultados_grid.seasonal_order[r], maxiter=500),\n",
    "                    )\n",
    "\n",
    "    metrica_m1, predicciones_m1 = backtesting_sarimax(\n",
    "                                            forecaster            = forecaster_1,\n",
    "                                            y                     = datos[columna],\n",
    "                                            initial_train_size    = int(len(datos_train)*0.8),\n",
    "                                            steps                 = size+12,\n",
    "                                            metric                = 'mean_absolute_error',\n",
    "                                            refit                 = True,\n",
    "                                            n_jobs                = \"auto\",\n",
    "                                            suppress_warnings_fit = True,\n",
    "                                            verbose               = False,\n",
    "                                            show_progress         = True\n",
    "                                        )\n",
    "\n",
    "    \n",
    "    return predicciones_m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo forecaster autorregresivo\n",
    "def prediccion_backtesting_forecasterAutoreg(datos_train,column,size,steps,param_grid,lags_grid,forecaster):\n",
    "\n",
    "    resultados_grid = grid_search_forecaster(\n",
    "                        forecaster         = forecaster,\n",
    "                        y                  = datos_train[column],\n",
    "                        param_grid         = param_grid,\n",
    "                        lags_grid          = lags_grid,\n",
    "                        steps              = steps,\n",
    "                        refit              = False,\n",
    "                        metric             = 'mean_squared_error',\n",
    "                        initial_train_size = int(len(datos_train)*0.8),\n",
    "                        fixed_train_size   = False,\n",
    "                        return_best        = True,\n",
    "                        n_jobs             = 'auto',\n",
    "                        verbose            = False\n",
    "                    )\n",
    "\n",
    "    # Predicciones\n",
    "    # ==============================================================================\n",
    "    predicciones = forecaster.predict(steps=size)\n",
    "\n",
    "    return predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo forecaster autorregresivo directo con regresor lineal con penalización Ridge devolviendo las predicciones\n",
    "def predicciones_backtesting_forecasterAutoregDirect(datos_train,column,steps,param_grid,lags_grid,forecaster):\n",
    "\n",
    "    resultados_grid = grid_search_forecaster(\n",
    "                        forecaster         = forecaster,\n",
    "                        y                  = datos_train[column],\n",
    "                        param_grid         = param_grid,\n",
    "                        lags_grid          = lags_grid,\n",
    "                        steps              = steps,\n",
    "                        refit              = False,\n",
    "                        metric             = 'mean_squared_error',\n",
    "                        initial_train_size = int(len(datos_train)*0.8),\n",
    "                        fixed_train_size   = False,\n",
    "                        return_best        = True,\n",
    "                        n_jobs             = 'auto',\n",
    "                        verbose            = False\n",
    "                    )\n",
    "\n",
    "    # Predicciones\n",
    "    # ==============================================================================\n",
    "    predicciones = forecaster.predict()\n",
    " \n",
    "    return predicciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el modelo de predicción prophet cuyos parámetros son unos datos de entrenamiento y otros de test y devolvemos las predicciones\n",
    "def pred_prophet_prediccion(data_train,column,size,frequ):\n",
    "    \n",
    "    data_train=data_train.reset_index()\n",
    "    data_train.rename(columns={data_train.columns[0] : 'ds', column: 'y'}, inplace=True)\n",
    "    model = Prophet()\n",
    "    model.fit(data_train)\n",
    "    \n",
    "    future = model.make_future_dataframe(periods=size,freq=frequ)\n",
    "    forecast=model.predict(future)\n",
    "    \n",
    "    y_pred=forecast['yhat'][len(data_train):].values\n",
    "    \n",
    "    return y_pred"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
